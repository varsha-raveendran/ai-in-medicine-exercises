{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 1c:** \n",
    "### **Multi-arm bandit problem - Tradeoff between exloration and exloitation**\n",
    "#### Week 8: Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **multi-armed bandit problem** is a classical reinforcement learning problem. It describes problems in which a fixed limited set of resources between competing (alternative) choices in a way that maximizes their expected gain, when each choice's properties are only partially known at the time of allocation, and may become better understood as time passes or by allocating resources to the choice.\n",
    "\n",
    "This is the original form of the k-armed bandit problem, so named by analogy to a slot machine, or “one-armed bandit,” except that it has k levers instead of one. Each action selection is like a play of one of the slot machine’s levers, and the rewards are the payoffs for hitting the jackpot.\n",
    "In the problem, each machine provides a random reward from a probability distribution specific to that machine, that is not known a-priori. The objective of the gambler is to maximize the sum of rewards earned through a sequence of lever pulls.\n",
    "The crucial tradeoff the gambler faces at each trial is between **exploitation** of the machine that has the highest expected payoff and **exploration** to get more information about the expected payoffs of the other machines.\n",
    "\n",
    "\n",
    "https://en.wikipedia.org/wiki/Multi-armed_bandit  \n",
    "Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd \n",
    "\n",
    "# Set font sizes for plotting\n",
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 14\n",
    "BIGGER_SIZE = 24\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some function for plotting and visualizing the results.\n",
    "\n",
    "def plot_average_rewards(bandit_reward_list, legend_list, title):\n",
    "\n",
    "    \"\"\"\n",
    "    Plot the average rewards (averaged over all episodes) per iteration of a list of bandits in one figure.\n",
    "\n",
    "    bandit_reward_list :    list of np arrays of length iter\n",
    "    legend_list :           list of method names for legend\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "    for i in range(len(bandit_reward_list)):\n",
    "        plt.plot(bandit_reward_list[i], label=legend_list[i])\n",
    "    plt.legend(bbox_to_anchor=(1.3, 0.5))\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Average Reward\")\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_selected_actions(bandit_action_list, legend_list, title):\n",
    "\n",
    "    \"\"\"\n",
    "    Plot how often the actions have been selected by each algorithm in a bar plot.\n",
    "\n",
    "    bandit_action_list :    list of np arrays of length iter\n",
    "    legend_list :           list of method names for legend\n",
    "    \"\"\"\n",
    "\n",
    "    color = ['b', 'g', 'r', 'm', 'k']\n",
    "    bins = np.linspace(0, k-1, k)\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "    for i in range(len(bandit_action_list)):\n",
    "        plt.bar(bins + i*0.33, bandit_action_list[i], \n",
    "                width = 0.33, color=color[i], \n",
    "                label=legend_list[i])\n",
    "    plt.legend(bbox_to_anchor=(1.2, 0.5))\n",
    "    plt.xlim([0,k])\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.xlabel(\"Action\")\n",
    "    plt.ylabel(\"Number of Actions Taken\")\n",
    "    plt.show()\n",
    "\n",
    "    opt_per = np.array(bandit_action_list) / iters * 100\n",
    "    df = pd.DataFrame(opt_per, index=legend_list,\n",
    "                    columns=[\"a = \" + str(x) for x in range(0, k)])\n",
    "    print(\"Percentage of actions selected:\")\n",
    "    print(df)\n",
    "\n",
    "\n",
    "def plot_arm_average_reward(ax, rewards_arm, reward_true, title):\n",
    "\n",
    "    \"\"\"\n",
    "    Plot the average rewards (averaged over all iterations) for all arms of a list of bandits in one figure.\n",
    "\n",
    "    ax :            axis for plotting\n",
    "    rewards_arm :   list of np arrays of length iter, estimated rewards for all arms at each iteration\n",
    "    reward_true :   true reward for each arm     \n",
    "    title :         method name\n",
    "    \"\"\"\n",
    "\n",
    "    colors = ['blue', 'purple', 'green', 'orange', 'red']\n",
    "    for i in range(k):\n",
    "        ax.plot(rewards_arm[:,i], label=f'M{i} estimated', color=colors[i])\n",
    "        ax.axhline(reward_true[i] , label=f'M{i} true', color=colors[i], linestyle='--')\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc='lower left')\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Value Estimate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `MultiArmedBandit` implements the multi-armed bandit problem with *k* arms.  \n",
    "Make yourself familiar with the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmedBandit():\n",
    "\n",
    "    \"\"\"\n",
    "    multi-bandit problem\n",
    "\n",
    "    Inputs\n",
    "    k:      number of arms (int)\n",
    "    eps:    probability of random action 0 < eps < 1 (float)\n",
    "    iters:  number of steps (int)\n",
    "    mu:     average rewards for each of the k arms.\n",
    "            'random': rewards selected from normal distribution with mean=0.\n",
    "            'sequence': rewards selected from normal distribution with mean from 0 to k-1 (ordered).\n",
    "            list or array of length k: user-defined values for the means.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, k, eps, iters, mu='random'):\n",
    "        # Number of arms\n",
    "        self.k = k\n",
    "\n",
    "        # Search probability\n",
    "        self.eps = eps\n",
    "        # Number of iterations\n",
    "        self.iters = iters\n",
    "        # Step count\n",
    "        self.t = 0\n",
    "        # Step count for each arm\n",
    "        self.k_t = np.zeros(k)\n",
    "        # Total mean reward\n",
    "        self.mean_reward = 0\n",
    "        self.reward = np.zeros(iters)\n",
    "        # Mean reward for each arm\n",
    "        self.k_mean_reward = np.zeros(k)\n",
    "        # self.k_rewards = np.zeros((k,iters))\n",
    "        self.k_rewards = []\n",
    "\n",
    "        if type(mu) == list or type(mu).__module__ == np.__name__:\n",
    "            # User-defined averages            \n",
    "            self.mu = np.array(mu)\n",
    "        elif mu == 'random':\n",
    "            # Draw means from probability distribution\n",
    "            self.mu = np.random.normal(0, 1, k)\n",
    "        elif mu == 'sequence':\n",
    "            # Increase the mean for each arm by one\n",
    "            self.mu = np.linspace(0, k-1, k)\n",
    "    \n",
    "    def pull(self):\n",
    "        \n",
    "        # action selection:\n",
    "\n",
    "        # Generate random number\n",
    "        p = np.random.rand()\n",
    "\n",
    "        # epsilon-greedy action\n",
    "        if self.eps == 0 and self.t == 0:\n",
    "            a = np.random.choice(self.k)\n",
    "        elif p < self.eps:\n",
    "            # Randomly select an action\n",
    "            a = np.random.choice(self.k)\n",
    "        else:\n",
    "            # Take greedy action\n",
    "            a = np.argmax(self.k_mean_reward)\n",
    "        \n",
    "\n",
    "        # reward for action a (sampled from reward distribution of action a)\n",
    "        reward = np.random.normal(self.mu[a], 1)\n",
    "        self.reward[self.t] = reward\n",
    "\n",
    "        # Update counts\n",
    "        self.t += 1  # total count\n",
    "        self.k_t[a] += 1  # count for each arm\n",
    "\n",
    "        # Update total reward\n",
    "        self.mean_reward = self.mean_reward + ( reward - self.mean_reward) / self.t\n",
    "\n",
    "        # Update reward for arm a (selected in the current pull)\n",
    "        self.k_mean_reward[a] = self.k_mean_reward[a] + ( reward - self.k_mean_reward[a]) / self.k_t[a]\n",
    "        self.k_rewards.append(copy.copy(self.k_mean_reward))\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        for i in range(self.iters):\n",
    "            self.pull()\n",
    "            self.reward[i] = self.mean_reward\n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "        # Resets results while keeping settings\n",
    "        self.t = 0\n",
    "        self.k_t = np.zeros(self.k)\n",
    "        self.mean_reward = 0\n",
    "        self.reward = np.zeros(self.iters)\n",
    "        self.k_mean_reward = np.zeros(self.k)\n",
    "        self.k_rewards = np.zeros((self.k,iters))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 2(a)**\n",
    "Briefly describe the *exploration-exploitation* dilemma. How is this addressed in the class `MultiArmedBandit`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------\n",
    "Run experiments with $1000$ test runs (episodes) of the Multi-armed bandit with $\\epsilon=0$, $\\epsilon=0.1$, $\\epsilon=0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For each of the three bandits, we set k=10, run 1,000 steps for each episode and run 1,000 episodes. \n",
    "After each episode, we will reset the bandits and copy the averages across the different bandits to keep things consistent.\n",
    "\"\"\"\n",
    "\n",
    "k = 10\n",
    "iters = 1000\n",
    "\n",
    "eps_0_rewards = np.zeros(iters)\n",
    "eps_01_rewards = np.zeros(iters)\n",
    "eps_1_rewards = np.zeros(iters)\n",
    "\n",
    "episodes = 1000\n",
    "# Run experiments\n",
    "for i in range(episodes):\n",
    "    \n",
    "    # Initialize three bandits with eps=0, eps=0.1, and eps=0.01 \n",
    "    eps_0 = MultiArmedBandit(k, 0, iters)\n",
    "    eps_01 = MultiArmedBandit(k, 0.01, iters, eps_0.mu.copy())\n",
    "    eps_1 = MultiArmedBandit(k, 0.1, iters, eps_0.mu.copy())\n",
    "    \n",
    "    # Run experiments\n",
    "    eps_0.run()\n",
    "    eps_01.run()\n",
    "    eps_1.run()\n",
    "    \n",
    "    # Update long-term averages\n",
    "    eps_0_rewards = eps_0_rewards + (\n",
    "        eps_0.reward - eps_0_rewards) / (i + 1)\n",
    "    eps_01_rewards = eps_01_rewards + (\n",
    "        eps_01.reward - eps_01_rewards) / (i + 1)\n",
    "    eps_1_rewards = eps_1_rewards + (\n",
    "        eps_1.reward - eps_1_rewards) / (i + 1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average $\\epsilon-greedy$ Rewards after 1000 Episodes\n",
    "plot_average_rewards([eps_0_rewards,eps_01_rewards,eps_1_rewards], \n",
    "                     [\"$\\epsilon=0$ (greedy)\", \"$\\epsilon=0.01$\", \"$\\epsilon=0.1$\"],\n",
    "                     \"Figure 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 2(b)**\n",
    "\n",
    "Describe and discuss the results which are displayed in Figure 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------\n",
    "Repeat the experiment with `k=4` and `mu='sequence'`.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "iters = 1000\n",
    "\n",
    "eps_0_rewards = np.zeros(iters)\n",
    "eps_01_rewards = np.zeros(iters)\n",
    "eps_1_rewards = np.zeros(iters)\n",
    "\n",
    "eps_0_selection = np.zeros(k)\n",
    "eps_01_selection = np.zeros(k)\n",
    "eps_1_selection = np.zeros(k)\n",
    "\n",
    "episodes = 1000\n",
    "# Run experiments\n",
    "for i in range(episodes):\n",
    "\n",
    "    # Initialize three bandits with eps=0, eps=0.1, and eps=0.01, and mu='sequence'\n",
    "    eps_0 = MultiArmedBandit(k, 0, iters, mu='sequence')\n",
    "    eps_01 = MultiArmedBandit(k, 0.01, iters, eps_0.mu.copy())\n",
    "    eps_1 = MultiArmedBandit(k, 0.1, iters, eps_0.mu.copy())\n",
    "    \n",
    "    # Run experiments\n",
    "    eps_0.run()\n",
    "    eps_01.run()\n",
    "    eps_1.run()\n",
    "    \n",
    "    # Update long-term averages\n",
    "    eps_0_rewards = eps_0_rewards + (\n",
    "        eps_0.reward - eps_0_rewards) / (i + 1)\n",
    "    eps_01_rewards = eps_01_rewards + (\n",
    "        eps_01.reward - eps_01_rewards) / (i + 1)\n",
    "    eps_1_rewards = eps_1_rewards + (\n",
    "        eps_1.reward - eps_1_rewards) / (i + 1)\n",
    "\n",
    "    # Average actions per episode\n",
    "    eps_0_selection = eps_0_selection + (\n",
    "        eps_0.k_t - eps_0_selection) / (i + 1)\n",
    "    eps_01_selection = eps_01_selection + (\n",
    "        eps_01.k_t - eps_01_selection) / (i + 1)\n",
    "    eps_1_selection = eps_1_selection + (\n",
    "        eps_1.k_t - eps_1_selection) / (i + 1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average $\\epsilon-greedy$ Rewards after 1000 Episodes\n",
    "plot_average_rewards([eps_0_rewards,eps_01_rewards,eps_1_rewards], \n",
    "                     [\"$\\epsilon=0$ (greedy)\", \"$\\epsilon=0.01$\", \"$\\epsilon=0.1$\"],\n",
    "                     \"Figure 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 2(c) (i)**\n",
    "Compare Figure 2 to Figure 1. Which differences do you observe?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions Selected by Each Algorithm\n",
    "plot_selected_actions([eps_0_selection, eps_01_selection, eps_1_selection], \n",
    "                      ['$\\epsilon=0$', '$\\epsilon=0.01$', '$\\epsilon=0.1$'],\n",
    "                      \"Figure 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 2(c) (ii)**\n",
    "Compare how often the correct action is taken in each of the three models (illustrated in Figure 3). Describe and discuss the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "Figure 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_0.k_rewards = np.array(eps_0.k_rewards)\n",
    "eps_01.k_rewards = np.array(eps_01.k_rewards)\n",
    "eps_1.k_rewards = np.array(eps_1.k_rewards)\n",
    "\n",
    "fig, ax = plt.subplots(1,3,figsize=(25,6))\n",
    "plot_arm_average_reward(ax[0], eps_0.k_rewards, eps_0.mu, '$\\epsilon=0$')\n",
    "plot_arm_average_reward(ax[1], eps_01.k_rewards, eps_01.mu, '$\\epsilon=0.01$')\n",
    "plot_arm_average_reward(ax[2], eps_1.k_rewards, eps_1.mu, '$\\epsilon=0.1$')\n",
    "\n",
    "fig.suptitle('Figure 4', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 2(c) (iii)**\n",
    "Figure 4 shows the average estimated rewards and the true reward for each arm of the three bandits. Compare and discuss the three subplots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "An improved version of the $\\epsilon$-*greedy* action selection method is $\\epsilon$-*decay* action selection (implemented in class `MultiArmedBandid_decay`).  \n",
    "Here, $\\epsilon$ is dependent on the time step $t$, e.g., $\\epsilon(t)=\\frac{1}{1+t\\beta}$ with $\\beta<1$ (use $\\beta=\\frac{1}{k}$). \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmBandit_decay():\n",
    "\n",
    "    \"\"\"\n",
    "    k-bandit problem\n",
    "\n",
    "    Inputs\n",
    "    k:      number of arms (int)\n",
    "    eps:    probability of random action 0 < eps < 1 (float)\n",
    "    iters:  number of steps (int)\n",
    "    mu:     average rewards for each of the k arms.\n",
    "            'random': rewards selected from normal distribution with mean=0.\n",
    "            'sequence': rewards selected from normal distribution with mean from 0 to k-1 (ordered).\n",
    "            list or array of length k: user-defined values for the means.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, k, iters, mu='random'):\n",
    "        # Number of arms\n",
    "        self.k = k\n",
    "\n",
    "        # Number of iterations\n",
    "        self.iters = iters\n",
    "        # Step count\n",
    "        self.t = 0\n",
    "        # Step count for each arm\n",
    "        self.k_t = np.zeros(k)\n",
    "        # Total mean reward\n",
    "        self.mean_reward = 0\n",
    "        self.reward = np.zeros(iters)\n",
    "        # Mean reward for each arm\n",
    "        self.k_mean_reward = np.zeros(k)\n",
    "        # self.k_rewards = np.zeros((k,iters))\n",
    "        self.k_rewards = []\n",
    "\n",
    "        if type(mu) == list or type(mu).__module__ == np.__name__:\n",
    "            # User-defined averages            \n",
    "            self.mu = np.array(mu)\n",
    "        elif mu == 'random':\n",
    "            # Draw means from probability distribution\n",
    "            self.mu = np.random.normal(0, 1, k)\n",
    "        elif mu == 'sequence':\n",
    "            # Increase the mean for each arm by one\n",
    "            self.mu = np.linspace(0, k-1, k)\n",
    "    \n",
    "    def pull(self):\n",
    "\n",
    "        # action selection:\n",
    "\n",
    "        # Generate random number\n",
    "        p = np.random.rand()\n",
    "\n",
    "        # epsilon-decay action\n",
    "        if p < 1 / (1 + self.t / self.k):\n",
    "            a = np.random.choice(self.k)\n",
    "        else:\n",
    "            # Take greedy action\n",
    "            a = np.argmax(self.k_mean_reward)\n",
    "\n",
    "        # reward for action a (sampled from reward distribution of action a)\n",
    "        reward = np.random.normal(self.mu[a], 1)\n",
    "        self.reward[self.t] = reward\n",
    "\n",
    "        # Update counts\n",
    "        self.t += 1\n",
    "        self.k_t[a] += 1\n",
    "\n",
    "        # Update total reward\n",
    "        self.mean_reward = self.mean_reward + ( reward - self.mean_reward) / self.t\n",
    "\n",
    "        # Update reward for arm a (selected in the current pull)\n",
    "        self.k_mean_reward[a] = self.k_mean_reward[a] + ( reward - self.k_mean_reward[a]) / self.k_t[a]\n",
    "        self.k_rewards.append(copy.copy(self.k_mean_reward))\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        for i in range(self.iters):\n",
    "            self.pull()\n",
    "            self.reward[i] = self.mean_reward\n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "        # Resets results while keeping settings\n",
    "        self.t = 0\n",
    "        self.k_t = np.zeros(self.k)\n",
    "        self.mean_reward = 0\n",
    "        self.reward = np.zeros(self.iters)\n",
    "        self.k_mean_reward = np.zeros(self.k)\n",
    "        self.k_rewards = np.zeros((self.k,iters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 2(d) (i)**\n",
    "In which sense does this improve the $\\epsilon$-*greedy* action selection? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "Run experiments with `k=4` and three bandits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "iters = 1000\n",
    "\n",
    "eps_0_rewards = np.zeros(iters)\n",
    "eps_1_rewards = np.zeros(iters)\n",
    "eps_d_rewards = np.zeros(iters)\n",
    "\n",
    "eps_0_selection = np.zeros(k)\n",
    "eps_1_selection = np.zeros(k)\n",
    "eps_d_selection = np.zeros(k)\n",
    "\n",
    "episodes = 1000\n",
    "# Run experiments\n",
    "for i in range(episodes):\n",
    "\n",
    "    # Initialize three bandits with eps=0, eps=0.1, and eps-decay\n",
    "    eps_0 = MultiArmedBandit(k, 0, iters, mu='sequence')\n",
    "    eps_1 = MultiArmedBandit(k, 0.1, iters, eps_0.mu.copy())\n",
    "    eps_d = MultiArmBandit_decay(k, iters, eps_0.mu.copy())\n",
    "    \n",
    "    # Run experiments\n",
    "    eps_0.run()\n",
    "    eps_1.run()\n",
    "    eps_d.run()\n",
    "    \n",
    "    # Update long-term averages\n",
    "    eps_0_rewards = eps_0_rewards + (\n",
    "        eps_0.reward - eps_0_rewards) / (i + 1)\n",
    "    eps_1_rewards = eps_1_rewards + (\n",
    "        eps_1.reward - eps_1_rewards) / (i + 1)\n",
    "    eps_d_rewards = eps_d_rewards + (\n",
    "        eps_d.reward - eps_d_rewards) / (i + 1)\n",
    "\n",
    "    # Average actions per episode\n",
    "    eps_0_selection = eps_0_selection + (\n",
    "        eps_0.k_t - eps_0_selection) / (i + 1)\n",
    "    eps_1_selection = eps_1_selection + (\n",
    "        eps_1.k_t - eps_1_selection) / (i + 1)\n",
    "    eps_d_selection = eps_d_selection + (\n",
    "        eps_d.k_t - eps_d_selection) / (i + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average epsilon-greedy/decay Rewards after 1000 Episodes\n",
    "plot_average_rewards([eps_0_rewards,eps_1_rewards,eps_d_rewards], \n",
    "                     [\"$\\epsilon=0$ (greedy)\", \"$\\epsilon=0.1$\", \"$\\epsilon-decay$\"],\n",
    "                     \"Figure 5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions Selected by Each Algorithm\n",
    "plot_selected_actions([eps_0_selection, eps_1_selection, eps_d_selection], \n",
    "                      ['$\\epsilon=0$', '$\\epsilon=0.1$', '$\\epsilon$-decay'],\n",
    "                      \"Figure 6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 2(d) (ii)**\n",
    "\n",
    "Discuss the results presented in Figures 5 and 6."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "21ad64d621597b49b60eb86a324141f8c655acebdbe808e43e3b54691c106b69"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('snow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
